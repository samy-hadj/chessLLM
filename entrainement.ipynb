{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-29T16:00:20.640134Z",
     "iopub.status.busy": "2024-11-29T16:00:20.639776Z",
     "iopub.status.idle": "2024-11-29T16:00:51.850177Z",
     "shell.execute_reply": "2024-11-29T16:00:51.849393Z",
     "shell.execute_reply.started": "2024-11-29T16:00:20.640101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def clean_moves(game_text):\n",
    "    cleaned_text = re.sub(r\"\\b\\d+\\.\\s*\", \"\", game_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def contains_evaluations(game_text):\n",
    "    return bool(re.search(r\"\\{ \\[%eval .*?\\] \\}\", game_text))\n",
    "\n",
    "def load_games(file_path, max_games, prefix=\"Début de la partie :\"):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    games = content.split(\"[Event \")\n",
    "    \n",
    "    games_text = []\n",
    "    i = 0\n",
    "    for game in games[1:]:\n",
    "        lines = game.splitlines()\n",
    "        moves = []\n",
    "        result = None\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line.endswith(\"]\") and line:\n",
    "                moves.append(line)\n",
    "\n",
    "        if moves:\n",
    "            game_text = \" \".join(moves)\n",
    "            cleaned_game_text = clean_moves(game_text)\n",
    "\n",
    "            if contains_evaluations(cleaned_game_text):\n",
    "                continue\n",
    "\n",
    "            game_with_prefix = f\"{prefix} {cleaned_game_text}\".strip()\n",
    "            games_text.append(game_with_prefix)\n",
    "        \n",
    "        i += 1\n",
    "        if i == max_games:\n",
    "            break\n",
    "\n",
    "    return games_text\n",
    "\n",
    "pgn_file_path = \"chess-files/lichess_db_standard_rated_2014-09.pgn\"\n",
    "games_text = load_games(pgn_file_path, max_games=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T16:00:56.338954Z",
     "iopub.status.busy": "2024-11-29T16:00:56.338621Z",
     "iopub.status.idle": "2024-11-29T16:00:56.345204Z",
     "shell.execute_reply": "2024-11-29T16:00:56.344187Z",
     "shell.execute_reply.started": "2024-11-29T16:00:56.338927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, games, tokenizer, max_length=1024):\n",
    "        self.games = games\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.games)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        game_text = self.games[idx]\n",
    "        encodings = self.tokenizer(\n",
    "            game_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = encodings[\"input_ids\"].squeeze()\n",
    "        attention_mask = encodings[\"attention_mask\"].squeeze()\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T16:00:58.156379Z",
     "iopub.status.busy": "2024-11-29T16:00:58.155651Z",
     "iopub.status.idle": "2024-11-29T16:01:02.949484Z",
     "shell.execute_reply": "2024-11-29T16:01:02.948460Z",
     "shell.execute_reply.started": "2024-11-29T16:00:58.156341Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde602a33bc94cbeb9027c96b150cf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632e3c6e6def4df6aa265b8635c0854b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413cdbd7af214499834c52398e1a8cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0997704b9af44dbebb891422d3f140b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada61b86391a43029b2d091876ec5afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T16:01:15.939493Z",
     "iopub.status.busy": "2024-11-29T16:01:15.939127Z",
     "iopub.status.idle": "2024-11-29T16:01:15.961052Z",
     "shell.execute_reply": "2024-11-29T16:01:15.960146Z",
     "shell.execute_reply.started": "2024-11-29T16:01:15.939464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, val_sequences = train_test_split(games_text, test_size=0.1)\n",
    "\n",
    "train_dataset = ChessDataset(train_sequences, tokenizer)\n",
    "eval_dataset = ChessDataset(val_sequences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T16:01:22.761845Z",
     "iopub.status.busy": "2024-11-29T16:01:22.761489Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212add3211d543b2bc1d3c793704b141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91463f292e14a42a68a7de90b9e5ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d24781d28b24e8f8c1f88999a086d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'appareil utilisé est : cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241129_160150-9n4nhlcq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/briacsix-epita/huggingface/runs/9n4nhlcq' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/briacsix-epita/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/briacsix-epita/huggingface' target=\"_blank\">https://wandb.ai/briacsix-epita/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/briacsix-epita/huggingface/runs/9n4nhlcq' target=\"_blank\">https://wandb.ai/briacsix-epita/huggingface/runs/9n4nhlcq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18149' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18149/22500 11:50:47 < 2:50:25, 0.43 it/s, Epoch 1.61/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.577700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.323600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.329800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.312600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.297100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.278500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.278500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.260300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.263900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.254600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.260900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.254700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.254600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.240200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.247200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=1000,\n",
    "    logging_steps=200,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1.7e-5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"L'appareil utilisé est : {device}\")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRAINEMENT FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T18:59:58.228967Z",
     "iopub.status.busy": "2024-11-28T18:59:58.228625Z",
     "iopub.status.idle": "2024-11-28T23:56:22.134019Z",
     "shell.execute_reply": "2024-11-28T23:56:22.131780Z",
     "shell.execute_reply.started": "2024-11-28T18:59:58.228939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'appareil utilisé est : cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6d9ec96a9a4b36b7175ea873b342b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113029833333283, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241128_190018-6yfhfbc7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/briacsix-epita/huggingface/runs/6yfhfbc7' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/briacsix-epita/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/briacsix-epita/huggingface' target=\"_blank\">https://wandb.ai/briacsix-epita/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/briacsix-epita/huggingface/runs/6yfhfbc7' target=\"_blank\">https://wandb.ai/briacsix-epita/huggingface/runs/6yfhfbc7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7553' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7553/22500 4:55:54 < 9:45:44, 0.43 it/s, Epoch 0.67/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.394600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.368500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.367700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.344100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.338800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.341500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.329300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.325900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.321700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.311600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.311300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappareil utilisé est : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Lancer le fine-tuning\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    n_positions=1024,               \n",
    "    n_embd=1024,                   \n",
    "    n_layer=24,                  \n",
    "    n_head=16,                    \n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2, \n",
    "    save_steps=1000,\n",
    "    logging_steps=200,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=2\n",
    "    learning_rate=1.5e-5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "from torch import cuda\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"L'appareil utilisé est : {device}\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST DU MODELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T22:00:34.548950Z",
     "iopub.status.busy": "2024-11-30T22:00:34.548690Z",
     "iopub.status.idle": "2024-11-30T22:00:34.787485Z",
     "shell.execute_reply": "2024-11-30T22:00:34.786292Z",
     "shell.execute_reply.started": "2024-11-30T22:00:34.548924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\briac\\Desktop\\EPITA\\ChessLLM\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chess\n",
    "import chess.pgn\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_path = \"models/best\"\n",
    "tokenizer_path = \"gpt2\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.svg\n",
    "from IPython.display import SVG\n",
    "import IPython.display\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def check_identical_scores(scores):\n",
    "    \"\"\"\n",
    "    Compare chaque sous-liste de scores pour vérifier si elles sont identiques.\n",
    "    \"\"\"\n",
    "    print(\"Vérification des sous-listes de scores...\")\n",
    "    identical_count = 0\n",
    "    for i in range(1, len(scores)):\n",
    "        if torch.allclose(scores[i], scores[i - 1], atol=1e-6):\n",
    "            identical_count += 1\n",
    "            print(f\"Les scores de l'étape {i} sont identiques à ceux de l'étape {i-1}.\")\n",
    "        else:\n",
    "            print(f\"Les scores de l'étape {i} diffèrent de ceux de l'étape {i-1}.\")\n",
    "    \n",
    "    print(f\"Nombre total de sous-listes identiques : {identical_count}/{len(scores)}\")\n",
    "    return identical_count\n",
    "\n",
    "\n",
    "def get_keypress():\n",
    "    return input(\"Appuyez sur Entrée pour continuer ou tapez 'q' pour quitter : \").strip()\n",
    "\n",
    "def decode_and_display_tokens(sequence, tokenizer):\n",
    "    print(f\"Décodage de la séquence : {sequence}\\n\")\n",
    "    \n",
    "    for i, token_id in enumerate(sequence):\n",
    "        token_text = tokenizer.decode([token_id], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Token {i+1}: '{token_text}' (ID: {token_id})\")\n",
    "\n",
    "\n",
    "def generate_candidate_moves(input_sequence, num_candidates=10, tokens_for_generation=5):\n",
    "    \"\"\"\n",
    "    Générer des séquences candidates avec leurs probabilités associées.\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(\n",
    "        input_sequence,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "    max_length = input_ids.size(1) + tokens_for_generation\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_candidates,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    sequences = output_sequences.sequences\n",
    "    scores = output_sequences.scores\n",
    "\n",
    "    candidate_moves = []\n",
    "    for idx, sequence in enumerate(sequences):\n",
    "\n",
    "        generated_tokens = sequence[len(input_ids[0]):]\n",
    "\n",
    "        next_move_tokens = []\n",
    "        space_count = 0\n",
    "\n",
    "        for token in generated_tokens:\n",
    "            token_text = tokenizer.decode([token], skip_special_tokens=True)\n",
    "            next_move_tokens.append(token_text)\n",
    "\n",
    "            if \" \" in token_text:\n",
    "                space_count += token_text.count(\" \")\n",
    "\n",
    "            if space_count == 2:\n",
    "                break\n",
    "\n",
    "        next_move = \"\".join(next_move_tokens)\n",
    "\n",
    "        second_space_index = next_move.find(\" \", next_move.find(\" \") + 1)\n",
    "        if second_space_index != -1 and second_space_index + 1 < len(next_move):\n",
    "            next_move = next_move[:second_space_index]\n",
    "\n",
    "\n",
    "        tokenized_move = tokenizer.encode(next_move, add_special_tokens=False)\n",
    "\n",
    "        step_scores = [scores[token_idx][idx] for token_idx in range(len(generated_tokens))]\n",
    "\n",
    "        if len(tokenized_move) > len(step_scores):\n",
    "            tokenized_move = tokenized_move[:len(step_scores)]\n",
    "\n",
    "        move_probability = calculate_probability(step_scores, tokenized_move)\n",
    "        candidate_moves.append((next_move, move_probability))\n",
    "\n",
    "    return candidate_moves\n",
    "\n",
    "\n",
    "def calculate_probability(scores, tokenized_move):\n",
    "    prob = 1.0\n",
    "    \n",
    "    for i, (token_id, step_score) in enumerate(zip(tokenized_move, scores)):\n",
    "        probabilities = torch.softmax(step_score, dim=-1)\n",
    "        \n",
    "        token_prob = probabilities[token_id].item()\n",
    "        prob *= token_prob\n",
    "\n",
    "        token_text = tokenizer.decode([token_id], skip_special_tokens=True)\n",
    "        \n",
    "    return prob\n",
    "\n",
    "def apply_move_to_board(board, move_text):\n",
    "    try:\n",
    "        move = board.parse_san(move_text)  # Convertir le texte en un coup compréhensible par python-chess\n",
    "        board.push(move)  # Appliquer le mouvement à l'échiquier\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def display_board(board):\n",
    "    clear_output(wait=True)  # Nettoyer la sortie précédente\n",
    "    svg_code = chess.svg.board(board=board, size=400)  # Ajuster la taille si nécessaire\n",
    "    display(SVG(svg_code))\n",
    "\n",
    "\n",
    "def test_chess_model(num_candidates=10, tokens_for_generation=5):\n",
    "    board = chess.Board()\n",
    "    input_sequence = \"Début de la partie :\"\n",
    "    \n",
    "    print(\"Appuyez sur Entrée pour générer un coup, ou 'q' pour quitter.\")\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        display_board(board)\n",
    "        print(\"\\nMove history:\", input_sequence)\n",
    "\n",
    "        candidates = generate_candidate_moves(input_sequence, num_candidates, tokens_for_generation)\n",
    "        print(\"\\nCandidates and probabilities:\")\n",
    "        for move, prob in candidates:\n",
    "            print(f\"{move.strip()}: {prob:.4f}\")\n",
    "\n",
    "        legal_moves = [(move.strip(), prob) for move, prob in candidates if apply_move_to_board(board.copy(), move.strip())]\n",
    "        if not legal_moves:\n",
    "            print(\"No valid moves generated. Stopping.\")\n",
    "            break\n",
    "\n",
    "        print(\"Appuyez sur Entrée pour continuer ou 'q' pour quitter.\")\n",
    "        key = get_keypress()\n",
    "\n",
    "        print(f\"Key pressed: {repr(key)}\")\n",
    "\n",
    "        if key.lower() == \"q\":\n",
    "            print(\"Arrêt de la partie.\")\n",
    "            break\n",
    "        elif key in [\"\\r\", \"\\n\", \"\"]:\n",
    "            best_move, _ = max(legal_moves, key=lambda x: x[1])\n",
    "\n",
    "            apply_move_to_board(board, best_move)\n",
    "            input_sequence += f\" {best_move}\"\n",
    "        else:\n",
    "            print(\"Appuyez sur Entrée pour continuer ou 'q' pour quitter.\")\n",
    "\n",
    "    print(str(board.is_game_over()))\n",
    "    print(\"\\nPartie terminée. Résultat:\", board.result())\n",
    "\n",
    "\n",
    "def test_chess_model_vs_stockfish(num_candidates=10, tokens_for_generation=5):\n",
    "\n",
    "    board = chess.Board()\n",
    "    input_sequence = \"Début de la partie :\"\n",
    "    stockfish.set_fen_position(board.fen())\n",
    "    \n",
    "    print(\"Appuyez sur Entrée pour jouer le prochain coup, ou 'q' pour quitter.\")\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        display_board(board)\n",
    "        print(\"\\nMove history:\", input_sequence)\n",
    "\n",
    "        if board.turn == chess.WHITE:\n",
    "            print(\"Tour du modèle.\")\n",
    "            candidates = generate_candidate_moves(input_sequence, num_candidates, tokens_for_generation)\n",
    "            print(\"\\nCandidates and probabilities:\")\n",
    "            for move, prob in candidates:\n",
    "                print(f\"{move.strip()}: {prob:.4f}\")\n",
    "\n",
    "            legal_moves = [(move.strip(), prob) for move, prob in candidates if apply_move_to_board(board.copy(), move.strip())]\n",
    "            if not legal_moves:\n",
    "                print(\"No valid moves generated. Stopping.\")\n",
    "                break\n",
    "\n",
    "            best_move, _ = max(legal_moves, key=lambda x: x[1])\n",
    "            apply_move_to_board(board, best_move)\n",
    "            input_sequence += f\" {best_move}\"\n",
    "        else:\n",
    "            print(\"Tour de Stockfish.\")\n",
    "            stockfish.set_fen_position(board.fen())\n",
    "            stockfish_move = stockfish.get_best_move()\n",
    "            if stockfish_move:\n",
    "                move = board.parse_uci(stockfish_move)\n",
    "                board.push(move)\n",
    "                input_sequence += f\" {board.san(move)}\"\n",
    "            else:\n",
    "                print(\"Stockfish n'a pas pu jouer de coup. Fin de partie.\")\n",
    "                break\n",
    "\n",
    "        if board.is_game_over():\n",
    "            print(\"\\nGame over!\")\n",
    "            print(\"Result:\", board.result())\n",
    "            break\n",
    "\n",
    "        key = get_keypress()\n",
    "        if key.lower() == \"q\":\n",
    "            print(\"Arrêt de la partie.\")\n",
    "            break\n",
    "\n",
    "    display_board(board)\n",
    "    print(\"\\nPartie terminée. Résultat:\", board.result())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores shape: 5 tokens, torch.Size([10, 50257]) per token\n",
      "\n",
      "Processing candidate sequence 1/10...\n",
      "test cut spaces:  N\n",
      "Prochain coup :  dxe5\n",
      "longueur du coup : 5\n",
      "Nombre de tokens dans 'tokenized_move': 3\n",
      "longueur des step_scores : 5\n",
      "\n",
      "Calcul des probabilités pour le coup : [288, 27705, 20] (en tokens)\n",
      "Token 1: ' d' (ID: 288) - Probabilité: 0.857855\n",
      "Token 2: 'xe' (ID: 27705) - Probabilité: 1.000000\n",
      "Token 3: '5' (ID: 20) - Probabilité: 1.000000\n",
      "Probabilité cumulative pour le coup : 0.857855\n",
      "\n",
      "\n",
      "Processing candidate sequence 2/10...\n",
      "test cut spaces:  N\n",
      "Prochain coup :  dxe5\n",
      "longueur du coup : 5\n",
      "Nombre de tokens dans 'tokenized_move': 3\n",
      "longueur des step_scores : 5\n",
      "\n",
      "Calcul des probabilités pour le coup : [288, 27705, 20] (en tokens)\n",
      "Token 1: ' d' (ID: 288) - Probabilité: 0.857855\n",
      "Token 2: 'xe' (ID: 27705) - Probabilité: 1.000000\n",
      "Token 3: '5' (ID: 20) - Probabilité: 1.000000\n",
      "Probabilité cumulative pour le coup : 0.857855\n",
      "\n",
      "\n",
      "Processing candidate sequence 3/10...\n",
      "test cut spaces:  f\n",
      "Prochain coup :  dxe5\n",
      "longueur du coup : 5\n",
      "Nombre de tokens dans 'tokenized_move': 3\n",
      "longueur des step_scores : 5\n",
      "\n",
      "Calcul des probabilités pour le coup : [288, 27705, 20] (en tokens)\n",
      "Token 1: ' d' (ID: 288) - Probabilité: 0.857855\n",
      "Token 2: 'xe' (ID: 27705) - Probabilité: 1.000000\n",
      "Token 3: '5' (ID: 20) - Probabilité: 1.000000\n",
      "Probabilité cumulative pour le coup : 0.857855\n",
      "\n",
      "\n",
      "Processing candidate sequence 4/10...\n",
      "test cut spaces:  f\n",
      "Prochain coup :  dxe5\n",
      "longueur du coup : 5\n",
      "Nombre de tokens dans 'tokenized_move': 3\n",
      "longueur des step_scores : 5\n",
      "\n",
      "Calcul des probabilités pour le coup : [288, 27705, 20] (en tokens)\n",
      "Token 1: ' d' (ID: 288) - Probabilité: 0.857855\n",
      "Token 2: 'xe' (ID: 27705) - Probabilité: 1.000000\n",
      "Token 3: '5' (ID: 20) - Probabilité: 1.000000\n",
      "Probabilité cumulative pour le coup : 0.857855\n",
      "\n",
      "\n",
      "Processing candidate sequence 5/10...\n",
      "test cut spaces:  N\n",
      "Prochain coup :  dxe5\n",
      "longueur du coup : 5\n",
      "Nombre de tokens dans 'tokenized_move': 3\n",
      "longueur des step_scores : 5\n",
      "\n",
      "Calcul des probabilités pour le coup : [288, 27705, 20] (en tokens)\n",
      "Token 1: ' d' (ID: 288) - Probabilité: 0.857855\n",
      "Token 2: 'xe' (ID: 27705) - Probabilité: 1.000000\n",
      "Token 3: '5' (ID: 20) - Probabilité: 1.000000\n",
      "Probabilité cumulative pour le coup : 0.857855\n",
      "\n",
      "\n",
      "Processing candidate sequence 6/10...\n",
      "test cut spaces:  ex\n",
      "Prochain coup :  c4\n",
      "longueur du coup : 3\n",
      "Nombre de tokens dans 'tokenized_move': 2\n",
      "longueur des step_scores : 5\n",
      "\n",
      "Calcul des probabilités pour le coup : [269, 19] (en tokens)\n",
      "Token 1: ' c' (ID: 269) - Probabilité: 0.075808\n",
      "Token 2: '4' (ID: 19) - Probabilité: 0.943188\n",
      "Probabilité cumulative pour le coup : 0.071501\n",
      "\n",
      "\n",
      "Processing candidate sequence 7/10...\n",
      "test cut spaces:  Q\n",
      "Prochain coup :  dxe5\n",
      "longueur du coup : 5\n",
      "Nombre de tokens dans 'tokenized_move': 3\n",
      "longueur des step_scores : 5\n",
      "\n",
      "Calcul des probabilités pour le coup : [288, 27705, 20] (en tokens)\n",
      "Token 1: ' d' (ID: 288) - Probabilité: 0.857855\n",
      "Token 2: 'xe' (ID: 27705) - Probabilité: 1.000000\n",
      "Token 3: '5' (ID: 20) - Probabilité: 1.000000\n",
      "Probabilité cumulative pour le coup : 0.857855\n",
      "\n",
      "\n",
      "Processing candidate sequence 8/10...\n",
      "test cut spaces:  f\n",
      "Prochain coup :  dxe5\n",
      "longueur du coup : 5\n",
      "Nombre de tokens dans 'tokenized_move': 3\n",
      "longueur des step_scores : 5\n",
      "\n",
      "Calcul des probabilités pour le coup : [288, 27705, 20] (en tokens)\n",
      "Token 1: ' d' (ID: 288) - Probabilité: 0.857855\n",
      "Token 2: 'xe' (ID: 27705) - Probabilité: 1.000000\n",
      "Token 3: '5' (ID: 20) - Probabilité: 1.000000\n",
      "Probabilité cumulative pour le coup : 0.857855\n",
      "\n",
      "\n",
      "Processing candidate sequence 9/10...\n",
      "test cut spaces:  Q\n",
      "Prochain coup :  dxe5\n",
      "longueur du coup : 5\n",
      "Nombre de tokens dans 'tokenized_move': 3\n",
      "longueur des step_scores : 5\n",
      "\n",
      "Calcul des probabilités pour le coup : [288, 27705, 20] (en tokens)\n",
      "Token 1: ' d' (ID: 288) - Probabilité: 0.857855\n",
      "Token 2: 'xe' (ID: 27705) - Probabilité: 1.000000\n",
      "Token 3: '5' (ID: 20) - Probabilité: 1.000000\n",
      "Probabilité cumulative pour le coup : 0.857855\n",
      "\n",
      "\n",
      "Processing candidate sequence 10/10...\n",
      "test cut spaces:  f\n",
      "Prochain coup :  dxe5\n",
      "longueur du coup : 5\n",
      "Nombre de tokens dans 'tokenized_move': 3\n",
      "longueur des step_scores : 5\n",
      "\n",
      "Calcul des probabilités pour le coup : [288, 27705, 20] (en tokens)\n",
      "Token 1: ' d' (ID: 288) - Probabilité: 0.857855\n",
      "Token 2: 'xe' (ID: 27705) - Probabilité: 1.000000\n",
      "Token 3: '5' (ID: 20) - Probabilité: 1.000000\n",
      "Probabilité cumulative pour le coup : 0.857855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sequence = \"Début de la partie : e4 e5 Nf3 Nc6 Bc4 Nf6 Ng5 d5 exd5 Na5 Bb5+ Qd7 Bxd7+ Bxd7 O-O Bg4 d3 Bxd1 Rxd1 Bc5 Re1 b5 c3 Bxf2+ Kh1 Bxe1\"\n",
    "candidates = generate_candidate_moves(input_sequence, num_candidates=10, tokens_for_generation=7)\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer le test interactif\n",
    "test_chess_model(num_candidates=30, tokens_for_generation=6)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6123479,
     "sourceId": 9956397,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6200802,
     "sourceId": 10061962,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
